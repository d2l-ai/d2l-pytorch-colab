{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Entrada e Saída de Arquivos\n",
    "\n",
    "Até agora, discutimos como processar dados e como\n",
    "para construir, treinar e testar modelos de *Deep Learning*.\n",
    "No entanto, em algum momento, esperamos ser felizes o suficiente\n",
    "com os modelos aprendidos que queremos\n",
    "para salvar os resultados para uso posterior em vários contextos\n",
    "(talvez até mesmo para fazer previsões na implantação).\n",
    "Além disso, ao executar um longo processo de treinamento,\n",
    "a prática recomendada é salvar resultados intermediários periodicamente (pontos de verificação)\n",
    "para garantir que não perdemos vários dias de computação\n",
    "se tropeçarmos no cabo de alimentação do nosso servidor.\n",
    "Portanto, é hora de aprender como carregar e armazenar\n",
    "ambos os vetores de peso individuais e modelos inteiros.\n",
    "Esta seção aborda ambos os problemas.\n",
    "\n",
    "## Carregando e Salvando Tensores\n",
    "\n",
    "Para tensores individuais, podemos diretamente\n",
    "invocar as funções `load` e `save`\n",
    "para ler e escrever respectivamente.\n",
    "Ambas as funções exigem que forneçamos um nome,\n",
    "e `save` requer como entrada a variável a ser salva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Agora podemos ler os dados do arquivo armazenado de volta na memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "Podemos armazenar uma lista de tensores e lê-los de volta na memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Podemos até escrever e ler um dicionário que mapeia\n",
    "de *strings* a tensores.\n",
    "Isso é conveniente quando queremos \n",
    "ler ou escrever todos os pesos em um modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Carregando e Salvando Parâmetros de Modelos\n",
    "\n",
    "Salvar vetores de peso individuais (ou outros tensores) é útil,\n",
    "mas fica muito tedioso se quisermos salvar\n",
    "(e depois carregar) um modelo inteiro.\n",
    "Afinal, podemos ter centenas de\n",
    "grupos de parâmetros espalhados por toda parte.\n",
    "Por esta razão, a estrutura de *Deep Learning* fornece funcionalidades integradas\n",
    "para carregar e salvar redes inteiras.\n",
    "Um detalhe importante a notar é que este\n",
    "salva o modelo *parâmetros* e não o modelo inteiro.\n",
    "Por exemplo, se tivermos um MLP de 3 camadas,\n",
    "precisamos especificar a arquitetura separadamente.\n",
    "A razão para isso é que os próprios modelos podem conter código arbitrário,\n",
    "portanto, eles não podem ser serializados naturalmente.\n",
    "Assim, para restabelecer um modelo, precisamos\n",
    "para gerar a arquitetura em código\n",
    "e carregue os parâmetros do disco.\n",
    "Vamos começar com nosso MLP familiar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "A seguir, armazenamos os parâmetros do modelo como um arquivo com o nome \"mlp.params\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Para recuperar o modelo, instanciamos um clone\n",
    "do modelo MLP original.\n",
    "Em vez de inicializar aleatoriamente os parâmetros do modelo,\n",
    "lemos os parâmetros armazenados no arquivo diretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Uma vez que ambas as instâncias têm os mesmos parâmetros de modelo,\n",
    "o resultado computacional da mesma entrada `X` deve ser o mesmo.\n",
    "Deixe-nos verificar isso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "## Sumário\n",
    "\n",
    "* As funções `save` e `load` podem ser usadas para executar E/S de arquivo para objetos tensores.\n",
    "* Podemos salvar e carregar todos os conjuntos de parâmetros de uma rede por meio de um dicionário de parâmetros.\n",
    "* Salvar a arquitetura deve ser feito em código e não em parâmetros.\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Mesmo se não houver necessidade de implantar modelos treinados em um dispositivo diferente, quais são os benefícios práticos de armazenar parâmetros de modelo?\n",
    "1. Suponha que desejamos reutilizar apenas partes de uma rede para serem incorporadas a uma rede de arquitetura diferente. Como você usaria, digamos, as duas primeiras camadas de uma rede anterior em uma nova rede?\n",
    "1. Como você salvaria a arquitetura e os parâmetros da rede? Que restrições você imporia à arquitetura?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussão](https://discuss.d2l.ai/t/61)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMzc3NjU2MDM4XX0=\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}