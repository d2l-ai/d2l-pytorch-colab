{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install d2l==0.17.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Implementação Concisa de Regressão Linear\n",
    ":label:`sec_linear_concise`\n",
    "\n",
    "\n",
    "Amplo e intenso interesse em *deep learning* nos últimos anos\n",
    "inspiraram empresas, acadêmicos e amadores\n",
    "para desenvolver uma variedade de estruturas de código aberto maduras\n",
    "para automatizar o trabalho repetitivo de implementação\n",
    "algoritmos de aprendizagem baseados em gradiente.\n",
    "Em :numref:`sec_linear_scratch`, contamos apenas com\n",
    "(i) tensores para armazenamento de dados e álgebra linear;\n",
    "e (ii) auto diferenciação para cálculo de gradientes.\n",
    "Na prática, porque iteradores de dados, funções de perda, otimizadores,\n",
    "e camadas de rede neural\n",
    "são tão comuns que as bibliotecas modernas também implementam esses componentes para nós.\n",
    "\n",
    "Nesta seção, (**mostraremos como implementar\n",
    "o modelo de regressão linear**) de:numref:`sec_linear_scratch`\n",
    "(**de forma concisa, usando APIs de alto nível**) de estruturas de *deep learning*.\n",
    "\n",
    "\n",
    "## Gerando the Dataset\n",
    "\n",
    "Para começar, vamos gerar o mesmo conjunto de dados como em\n",
    ":numref:`sec_linear_scratch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## Lendo o Dataset\n",
    "\n",
    "Em vez de usar nosso próprio iterador,\n",
    "podemos [**chamar a API existente em uma estrutura para ler os dados.**]\n",
    "Passamos *`features`* e *`labels`* como argumentos e especificamos *`batch_size`*\n",
    "ao instanciar um objeto iterador de dados.\n",
    "Além disso, o valor booleano `is_train`\n",
    "indica se ou não\n",
    "queremos que o objeto iterador de dados embaralhe os dados\n",
    "em cada época (passe pelo conjunto de dados).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Now we can use `data_iter` in much the same way as we called\n",
    "the `data_iter` function in :numref:`sec_linear_scratch`.\n",
    "To verify that it is working, we can read and print\n",
    "the first minibatch of examples.\n",
    "Comparing with :numref:`sec_linear_scratch`,\n",
    "here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4291,  0.1270],\n",
       "         [ 1.7995, -1.2012],\n",
       "         [ 0.9239, -0.7505],\n",
       "         [ 1.3561, -0.4303],\n",
       "         [ 0.6144, -0.5138],\n",
       "         [-1.0876, -0.8626],\n",
       "         [ 1.1090,  3.4219],\n",
       "         [-1.6905,  0.1326],\n",
       "         [ 0.6009,  0.9365],\n",
       "         [ 0.1519, -1.1885]]),\n",
       " tensor([[ 4.6351],\n",
       "         [11.8713],\n",
       "         [ 8.5979],\n",
       "         [ 8.3702],\n",
       "         [ 7.1819],\n",
       "         [ 4.9432],\n",
       "         [-5.2131],\n",
       "         [ 0.3592],\n",
       "         [ 2.2248],\n",
       "         [ 8.5418]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Definindo o Modelo\n",
    "\n",
    "\n",
    "Quando implementamos a regressão linear do zero\n",
    "em :numref:`sec_linear_scratch`,\n",
    "definimos nossos parâmetros de modelo explicitamente\n",
    "e codificamos os cálculos para produzir saída\n",
    "usando operações básicas de álgebra linear.\n",
    "Você *deveria* saber como fazer isso.\n",
    "Mas quando seus modelos ficam mais complexos,\n",
    "e uma vez que você tem que fazer isso quase todos os dias,\n",
    "você ficará feliz com a ajuda.\n",
    "A situação é semelhante a codificar seu próprio blog do zero.\n",
    "Fazer uma ou duas vezes é gratificante e instrutivo,\n",
    "mas você seria um péssimo desenvolvedor da web\n",
    "se toda vez que você precisava de um blog você passava um mês\n",
    "reinventando tudo.\n",
    "\n",
    "Para operações padrão, podemos [**usar as camadas predefinidas de uma estrutura,**]\n",
    "o que nos permite focar especialmente\n",
    "nas camadas usadas para construir o modelo\n",
    "em vez de ter que se concentrar na implementação.\n",
    "Vamos primeiro definir uma variável de modelo `net`,\n",
    "que se refere a uma instância da classe `Sequential`.\n",
    "A classe `Sequential` define um contêiner\n",
    "para várias camadas que serão encadeadas.\n",
    "Dados dados de entrada, uma instância `Sequential` passa por\n",
    "a primeira camada, por sua vez passando a saída\n",
    "como entrada da segunda camada e assim por diante.\n",
    "No exemplo a seguir, nosso modelo consiste em apenas uma camada,\n",
    "portanto, não precisamos realmente de `Sequencial`.\n",
    "Mas como quase todos os nossos modelos futuros\n",
    "envolverão várias camadas,\n",
    "vamos usá-lo de qualquer maneira apenas para familiarizá-lo\n",
    "com o fluxo de trabalho mais padrão.\n",
    "\n",
    "Lembre-se da arquitetura de uma rede de camada única, conforme mostrado em :numref:`fig_single_neuron`.\n",
    "Diz-se que a camada está *totalmente conectada*\n",
    "porque cada uma de suas entradas está conectada a cada uma de suas saídas\n",
    "por meio de uma multiplicação de matriz-vetor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    ": begin_tab: `pytorch`\n",
    "No PyTorch, a camada totalmente conectada é definida na classe `Linear`. Observe que passamos dois argumentos para `nn.Linear`. O primeiro especifica a dimensão do recurso de entrada, que é 2, e o segundo é a dimensão do recurso de saída, que é um escalar único e, portanto, 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# `nn` is an abbreviation for neural networks\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## Inicializando os Parâmetros do Modelo\n",
    "\n",
    "\n",
    "Antes de usar `net`, precisamos (**inicializar os parâmetros do modelo,**)\n",
    "como os pesos e *bias* no modelo de regressão linear.\n",
    "As estruturas de *deep learning* geralmente têm uma maneira predefinida de inicializar os parâmetros.\n",
    "Aqui especificamos que cada parâmetro de peso\n",
    "deve ser amostrado aleatoriamente a partir de uma distribuição normal\n",
    "com média 0 e desvio padrão 0,01.\n",
    "O parâmetro bias será inicializado em zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "As we have specified the input and output dimensions when constructing `nn.Linear`. Now we access the parameters directly to specify their initial values. We first locate the layer by `net[0]`, which is the first layer in the network, and then use the `weight.data` and `bias.data` methods to access the parameters. Next we use the replace methods `normal_` and `fill_` to overwrite parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "## Definindo a Função de Perda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**A classe `MSELoss` calcula o erro quadrático médio, também conhecido como norma $ L_2 $ quadrada.**]\n",
    "Por padrão, ela retorna a perda média sobre os exemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Definindo o Algoritmo de Otimização\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "O gradiente descendente estocástico de *minibatch* é uma ferramenta padrão\n",
    "para otimizar redes neurais\n",
    "e, portanto, PyTorch o suporta ao lado de uma série de\n",
    "variações deste algoritmo no módulo `optim`.\n",
    "Quando nós (**instanciamos uma instância `SGD`,**)\n",
    "iremos especificar os parâmetros para otimizar\n",
    "(podem ser obtidos de nossa rede via `net.parameters ()`), com um dicionário de hiperparâmetros\n",
    "exigido por nosso algoritmo de otimização.\n",
    "O gradiente descendente estocástico de *minibatch* requer apenas que definamos o valor `lr`, que é definido como 0,03 aqui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "## Treinamento\n",
    "\n",
    "\n",
    "Você deve ter notado que expressar nosso modelo por meio\n",
    "APIs de alto nível de uma estrutura de *deep learning*\n",
    "requer comparativamente poucas linhas de código.\n",
    "Não tivemos que alocar parâmetros individualmente,\n",
    "definir nossa função de perda ou implementar o gradiente descendente estocástico de *minibatch*.\n",
    "Assim que começarmos a trabalhar com modelos muito mais complexos,\n",
    "as vantagens das APIs de alto nível aumentarão consideravelmente.\n",
    "No entanto, uma vez que temos todas as peças básicas no lugar,\n",
    "[**o loop de treinamento em si é surpreendentemente semelhante\n",
    "ao que fizemos ao implementar tudo do zero.**]\n",
    "\n",
    "Para refrescar sua memória: para anguns números de épocas,\n",
    "faremos uma passagem completa sobre o conjunto de dados (*`train_data`*),\n",
    "pegando iterativamente um *minibatch* de entradas\n",
    "e os *labels* de verdade fundamental correspondentes.\n",
    "Para cada *minibatch*, passamos pelo seguinte ritual:\n",
    "\n",
    "* Gerar previsões chamando `net (X)` e calcular a perda `l` (a propagação direta).\n",
    "* Calcular gradientes executando a retropropagação.\n",
    "* Atualizar os parâmetros do modelo invocando nosso otimizador.\n",
    "\n",
    "Para uma boa medida, calculamos a perda após cada época e a imprimimos para monitorar o progresso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000239\n",
      "epoch 2, loss 0.000098\n",
      "epoch 3, loss 0.000099\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "Abaixo, nós [**comparamos os parâmetros do modelo aprendidos pelo treinamento em dados finitos\n",
    "e os parâmetros reais**] que geraram nosso *dataset*.\n",
    "Para acessar os parâmetros,\n",
    "primeiro acessamos a camada que precisamos de `net`\n",
    "e, em seguida, acessamos os pesos e a polarização dessa camada.\n",
    "Como em nossa implementação do zero,\n",
    "observe que nossos parâmetros estimados são\n",
    "perto de suas contrapartes verdadeiras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([0.0004, 0.0001])\n",
      "error in estimating b: tensor([-0.0005])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "## Resumo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "* Usando as APIs de alto nível do PyTorch, podemos implementar modelos de forma muito mais concisa.\n",
    "* No PyTorch, o módulo `data` fornece ferramentas para processamento de dados, o módulo` nn` define um grande número de camadas de rede neural e funções de perda comuns.\n",
    "* Podemos inicializar os parâmetros substituindo seus valores por métodos que terminam com `_`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "## Exercícios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. Se substituirmos `nn.MSELoss (*reduction* = 'sum')` por `nn.MSELoss ()`, como podemos alterar a taxa de aprendizagem para que o código se comporte de forma idêntica. Por quê?\n",
    "1. Revise a documentação do PyTorch para ver quais funções de perda e métodos de inicialização são fornecidos. Substitua a perda pela perda de Huber.\n",
    "1. Como você acessa o gradiente de `net[0].weight`?\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbLTUxMjc1MTc4MiwxMzgxNzE4MjMxLDE1MT\n",
    "Y2Nzk5ODgsLTMwMDU2OTMzNSwtNTUxNzY3NTUsMzYzNjY2MzMs\n",
    "LTY1Mjk5MTk1OCwtMjE0NTk5NTMwN119\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}